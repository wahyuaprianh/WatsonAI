{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe11c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "author: Elena Lowery\n",
    "\n",
    "This code sample shows how to invoke Large Language Models (LLMs) deployed in watsonx.ai.\n",
    "Documentation: https://ibm.github.io/watson-machine-learning-sdk/foundation_models.html\n",
    "You will need to provide your IBM Cloud API key and a watonx.ai project id  (any project)\n",
    "for accessing watsonx.ai in a .env file\n",
    "This example shows simple use cases without comprehensive prompt tuning\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0f7c2",
   "metadata": {},
   "source": [
    "Install the wml api your Python env prior to running this example:\n",
    "pip install ibm-watson-machine-learning\n",
    "pip install ibm-cloud-sdk-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c7995",
   "metadata": {},
   "source": [
    "In non-Anaconda Python environments, you may also need to install dotenv\n",
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032587c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For invocation of LLM with REST API\n",
    "import requests, json\n",
    "from ibm_cloud_sdk_core import IAMTokenManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7faf96d",
   "metadata": {},
   "source": [
    "Important: hardcoding the API key in Python code is not a best practice. We are using\n",
    "this approach for the ease of demo setup. In a production application these variables\n",
    "can be stored in an .env or a properties file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the hosted LLMs is hardcoded because at this time all LLMs share the same endpoint\n",
    "url = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4caea15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# These global variables will be updated in get_credentials() functions\n",
    "watsonx_project_id = \"\"\n",
    "# Replace with your IBM Cloud key\n",
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70725685",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_credentials():\n",
    "\n",
    "    load_dotenv(dotenv_path=\"file.env\")\n",
    "\n",
    "    # Update the global variables that will be used for authentication in another function\n",
    "    globals()[\"api_key\"] = os.getenv(\"api_key\", None)\n",
    "    globals()[\"watsonx_project_id\"] = os.getenv(\"project_id\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd02a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# The get_model function creates an LLM model object with the specified parameters\n",
    "def get_model(model_type,max_tokens,min_tokens,decoding,temperature):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": url\n",
    "        },\n",
    "        project_id=watsonx_project_id\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4562f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_list_of_complaints():\n",
    "\n",
    "    # Look up parameters in documentation:\n",
    "    # https://ibm.github.io/watson-machine-learning-sdk/foundation_models.html#\n",
    "\n",
    "    # You can specify any prompt and change parameters for different runs\n",
    "\n",
    "    # If you want the end user to have a choice of the number of tokens in the output as well as decoding\n",
    "    # and temperature, you can parameterize these values\n",
    "\n",
    "    model_type = ModelTypes.LLAMA_2_13B_CHAT\n",
    "    max_tokens = 100\n",
    "    min_tokens = 50\n",
    "    decoding = DecodingMethods.GREEDY\n",
    "    # Temperature will be ignored if GREEDY is used\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = get_model(model_type,max_tokens,min_tokens,decoding, temperature)\n",
    "\n",
    "    complaint = f\"\"\"\n",
    "            I just tried to book a flight on your incredibly slow website.  All \n",
    "            the times and prices were confusing.  I liked being able to compare \n",
    "            the amenities in economy with business class side by side.  But I \n",
    "            never got to reserve a seat because I didn't understand the seat map.  \n",
    "            Next time, I'll use a travel agent!\n",
    "            \"\"\"\n",
    "\n",
    "    # Hardcoding prompts in a script is not best practice. We are providing this code sample for simplicity of\n",
    "    # understanding\n",
    "\n",
    "    prompt_get_complaints = f\"\"\"\n",
    "    From the following customer complaint, extract 3 factors that caused the customer to be unhappy. \n",
    "    Put each factor on a new line. \n",
    "\n",
    "    Customer complaint:{complaint}\n",
    "\n",
    "    Numbered list of all the factors that caused the customer to be unhappy:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Invoke the model and print the results\n",
    "    generated_response = model.generate(prompt=prompt_get_complaints)\n",
    "    # WML API returns a dictionary object. Generated response is a list object that contains generated text\n",
    "    # as well as several other items such as token count and seed\n",
    "    # We recommmend that you put a breakpoint on this line and example the result object\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "    print(\"Prompt: \" + prompt_get_complaints)\n",
    "    print(\"List of complaints: \" + generated_response['results'][0]['generated_text'])\n",
    "    print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cc198",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def answer_questions():\n",
    "\n",
    "    # Look up parameters in documentation:\n",
    "    # https://ibm.github.io/watson-machine-learning-sdk/foundation_models.html#\n",
    "\n",
    "    # You can specify any prompt and change parameters for different runs\n",
    "\n",
    "    # If you want the end user to have a choice of the number of tokens in the output as well as decoding\n",
    "    # and temperature, you can parameterize these values\n",
    "\n",
    "    final_prompt = \"Write a paragraph about the capital of France.\"\n",
    "    model_type = ModelTypes.FLAN_UL2\n",
    "    max_tokens = 300\n",
    "    min_tokens = 50\n",
    "    decoding = DecodingMethods.SAMPLE\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = get_model(model_type,max_tokens,min_tokens,decoding, temperature)\n",
    "    # Invoke the model and print the results\n",
    "    generated_response = model.generate(prompt=final_prompt)\n",
    "    # WML API returns a dictionary object. Generated response is a list object that contains generated text\n",
    "    # as well as several other items such as token count and seed\n",
    "    # We recommmend that you put a breakpoint on this line and example the result object\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "    print(\"Question/request: \" + final_prompt)\n",
    "    print(\"Answer: \" + generated_response['results'][0]['generated_text'])\n",
    "    print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f1a12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def invoke_with_REST():\n",
    "\n",
    "    rest_url =\"https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2023-05-29\"\n",
    "\n",
    "    access_token = get_auth_token()\n",
    "\n",
    "    model_type = \"google/flan-ul2\"\n",
    "    max_tokens = 300\n",
    "    min_tokens = 50\n",
    "    decoding = \"sample\"\n",
    "    temperature = 0.7\n",
    "\n",
    "    final_prompt = \"Write a paragraph about the capital of France.\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + access_token\n",
    "        }\n",
    "\n",
    "    data = {\n",
    "        \"model_id\": model_type,\n",
    "        \"input\": final_prompt,\n",
    "        \"parameters\": {\n",
    "            \"decoding_method\": decoding,\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"min_new_tokens\": min_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"stop_sequences\": [\".\"],\n",
    "            },\n",
    "        \"project_id\": watsonx_project_id\n",
    "    }\n",
    "\n",
    "    response = requests.post(rest_url, headers=headers, data=json.dumps(data))\n",
    "    generated_response = response.json()['results'][0]['generated_text']\n",
    "\n",
    "    print(\"--------------------------Invocation with REST-------------------------------------------\")\n",
    "    print(\"Question/request: \" + final_prompt)\n",
    "    print(\"Answer: \" + generated_response)\n",
    "    print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01252a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_auth_token():\n",
    "\n",
    "    # Access token is required for REST invocation of the LLM\n",
    "    access_token = IAMTokenManager(apikey=api_key,url=\"https://iam.cloud.ibm.com/identity/token\").get_token()\n",
    "    return access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8568d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demo_LLM_invocation():\n",
    "\n",
    "    # Load the api key and project id\n",
    "    get_credentials()\n",
    "\n",
    "    # Show examples of 2 use cases/prompts\n",
    "    answer_questions()\n",
    "    get_list_of_complaints()\n",
    "\n",
    "    # Simple prompt - invoked with the REST API\n",
    "    invoke_with_REST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff979865",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_LLM_invocation()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
